{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4905b2",
   "metadata": {},
   "source": [
    "# Demonstrate and profile insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b829f2",
   "metadata": {},
   "source": [
    "**Set environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38168487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "### Set environment\n",
    "###++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "### import common packages\n",
    "import numpy  as np\n",
    "import itertools as it\n",
    "import sys, os, gzip\n",
    "from   functools import reduce\n",
    "\n",
    "### update print\n",
    "from functools import partial\n",
    "print = partial(print, flush=True)\n",
    "\n",
    "### set working directories\n",
    "FD_RES = \"/gpfs/fs1/data/reddylab/Kuei/out/proj_combeffect\"\n",
    "\n",
    "### import specific packages\n",
    "import sqlite3\n",
    "# https://stackoverflow.com/questions/49456158/integer-in-python-pandas-becomes-blob-binary-in-sqlite\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "\n",
    "### import packages for benchmark performance\n",
    "import cProfile, pstats, time, timeit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b938f77",
   "metadata": {},
   "source": [
    "**Global variables of database and file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9554b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global variables:\n",
      "Chromsome:   chr17\n",
      "Database:    /gpfs/fs1/data/reddylab/Kuei/out/proj_combeffect/database/test_insert_chr17.db\n",
      "Input file:  /gpfs/fs1/data/reddylab/Kuei/out/proj_combeffect/nuc/Input1_20x/chr17.bed.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "### parse arguments\n",
    "###++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "CHROM   = \"chr17\"                          #args.chrom\n",
    "FD_OUT  = os.path.join(FD_RES, \"database\") #args.fout\n",
    "FD_INP  = os.path.join(FD_RES, \"nuc\")      #args.finp\n",
    "PREFIX  = \"test_insert\"                    #args.prefix\n",
    "VERBOSE = True                             #args.verbose\n",
    "\n",
    "#######################################################\n",
    "### Global varialbes and I/O\n",
    "###++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "### file path of fragment database\n",
    "fdiry  = FD_OUT\n",
    "fname  = f\"{PREFIX}_{CHROM}.db\"\n",
    "FP_DTB = os.path.join(fdiry, fname)\n",
    "\n",
    "### file path of fragment table\n",
    "sample  = \"Input1_20x\"\n",
    "fdiry   = os.path.join(FD_INP, sample)\n",
    "fname   = \"chr17.bed.gz\"\n",
    "FP_FRG  = os.path.join(fdiry, fname)\n",
    "\n",
    "### show info\n",
    "if (VERBOSE):\n",
    "    print(\"Global variables:\")\n",
    "    print(\"Chromsome:  \", CHROM)\n",
    "    print(\"Database:   \", FP_DTB)\n",
    "    print(\"Input file: \", FP_FRG)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d911e91",
   "metadata": {},
   "source": [
    "## Setup helper function for reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c392c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "### Helper functions\n",
    "### ++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "### helper function to get a chunk of file\n",
    "def get_chunks(gen, rows=10):\n",
    "    \"\"\"Divides the data into chunks with size as rows\"\"\"\n",
    "    iterable = iter(gen)\n",
    "    while True:\n",
    "        x = list(it.islice(iterable, rows))\n",
    "        if not x:\n",
    "            return\n",
    "        yield x\n",
    "\n",
    "### helper function to process each row\n",
    "def prep_line(line):\n",
    "    \"\"\"Function to process each line\"\"\"\n",
    "    ### Decode\n",
    "    lst = line.decode('ASCII').strip().split('\\t') \n",
    "\n",
    "    ### parse info\n",
    "    key = \"_\".join(lst[0:3])\n",
    "    val = lst[0:3] + lst[4:-1]\n",
    "    return [key] + val\n",
    "\n",
    "def gen_line(file, n_chunksize=None, n_lines=None):\n",
    "    \"\"\"generate lines or chunks of lines from the file\"\"\"\n",
    "    ### remove file header\n",
    "    header = file.readline()\n",
    "    lines  = file\n",
    "    \n",
    "    ### preprocess each line\n",
    "    fun = prep_line\n",
    "    gen = map(fun, lines)\n",
    "    \n",
    "    ### set number of lines read if specified\n",
    "    if n_lines is not None:\n",
    "        gen = it.islice(gen, n_lines)\n",
    "    \n",
    "    ### set chunks if specified\n",
    "    if n_chunksize is not None:\n",
    "        gen = get_chunks(gen, n_chunksize)\n",
    "\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee98eb9",
   "metadata": {},
   "source": [
    "## Setup SQL query and functions for creating and inserting into fragment table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac24d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "### Set SQL query\n",
    "### ++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "query_reset_table = \"DROP TABLE IF EXISTS Fragment\"\n",
    "\n",
    "query_table_frag = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Fragment(\n",
    "        fragment TEXT PRIMARY KEY, \n",
    "        chrom    TEXT,\n",
    "        start    INTEGER,\n",
    "        end      INTEGER,\n",
    "        pct_at   REAL,\n",
    "        pct_gc   REAL,\n",
    "        num_A    INTEGER,\n",
    "        num_C    INTEGER,\n",
    "        num_G    INTEGER,\n",
    "        num_T    INTEGER,\n",
    "        num_N    INTEGER,\n",
    "        num_oth  INTEGER\n",
    "    );\"\"\")\n",
    "\n",
    "query_table_auto = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Fragment(\n",
    "        fragment TEXT, \n",
    "        chrom    TEXT,\n",
    "        start    INTEGER,\n",
    "        end      INTEGER,\n",
    "        pct_at   REAL,\n",
    "        pct_gc   REAL,\n",
    "        num_A    INTEGER,\n",
    "        num_C    INTEGER,\n",
    "        num_G    INTEGER,\n",
    "        num_T    INTEGER,\n",
    "        num_N    INTEGER,\n",
    "        num_oth  INTEGER\n",
    "    );\"\"\")\n",
    "\n",
    "query_insert = (\"\"\"\n",
    "    INSERT OR IGNORE INTO Fragment\n",
    "        (fragment, chrom, start, end, pct_at, pct_gc,\n",
    "         num_A, num_C, num_G, num_T, num_N, num_oth) \n",
    "    VALUES \n",
    "        (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb9726c-c3c1-473e-86e0-a664b0b5748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_insert_line_by_line(cursor, query, lines):\n",
    "    \"\"\"the core function to profile for line-by-line insertion\"\"\"\n",
    "    for line in lines:\n",
    "        cursor = cursor.execute(query, line)\n",
    "    return cursor\n",
    "\n",
    "def fun_insert_line_by_chunk(cursor, query, chunks):\n",
    "    \"\"\"the core function to profile for batch insertion\"\"\"\n",
    "    for chunk in chunks:\n",
    "        cursor = cursor.executemany(query, chunk)\n",
    "    return cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2762e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "### Set database function\n",
    "### ++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "def refresh(query_table, fpath_database):\n",
    "    \"\"\"\n",
    "    Helper function to refresh the database by \n",
    "    deleting original table and create a new one\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(fpath_database) as conn:\n",
    "        ### init\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        ### reset table\n",
    "        query  = query_reset_table\n",
    "        cursor = cursor.execute(query)\n",
    "\n",
    "        ### create table\n",
    "        query  = query_table\n",
    "        cursor = cursor.execute(query)\n",
    "        \n",
    "def insert_line_by_line(n_lines, query_table, fpath_database, fpath_table):\n",
    "    \"\"\"Helper function to insert rows into table line by line\"\"\"\n",
    "    \n",
    "    ### init a new table\n",
    "    refresh(query_table, fpath_database)\n",
    "    \n",
    "    with sqlite3.connect(fpath_database) as conn:\n",
    "        ### initiation\n",
    "        cursor = conn.cursor()\n",
    "        query  = query_insert\n",
    "        \n",
    "        ### read file\n",
    "        with gzip.open(fpath_table, \"rb\") as file:\n",
    "            \n",
    "            ### generate lines\n",
    "            lines = gen_line(file, n_lines=n_lines)\n",
    "\n",
    "            ### insert line by line\n",
    "            #for line in lines:\n",
    "            #    cursor.execute(query, line)\n",
    "            cursor = fun_insert_line_by_line(cursor, query, lines)\n",
    "                \n",
    "def insert_line_by_chunk(n_lines, n_chunksize, query_table, fpath_database, fpath_table):\n",
    "    \"\"\"Helper function to insert rows into table by chunk\"\"\"\n",
    "    ### init a new table\n",
    "    refresh(query_table, fpath_database)\n",
    "    \n",
    "    with sqlite3.connect(fpath_database) as conn:\n",
    "        ### initiation\n",
    "        cursor = conn.cursor()\n",
    "        query  = query_insert\n",
    "        \n",
    "        ### read file\n",
    "        with gzip.open(fpath_table, \"rb\") as file:\n",
    "            \n",
    "            ### generate chunks\n",
    "            chunks = gen_line(file, n_chunksize=n_chunksize, n_lines=n_lines)\n",
    "            \n",
    "            ### insert chunk by chunk\n",
    "            #for chunk in chunks:\n",
    "            #    cursor.executemany(query, chunk)\n",
    "            cursor = fun_insert_line_by_chunk(cursor, query, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af23be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_table_size(n_lines, fpath_database=FP_DTB):\n",
    "    \"\"\"count the number of rows/lines in a table created in a database\"\"\"\n",
    "    with sqlite3.connect(fpath_database) as conn:\n",
    "        ### initiation\n",
    "        cursor = conn.cursor()\n",
    "        query  = \"select count(*) from Fragment\"\n",
    "        \n",
    "        ### get the table size of table\n",
    "        cursor = cursor.execute(query)\n",
    "        counts = cursor.fetchall()\n",
    "        counts = counts[0][0]\n",
    "    \n",
    "    if counts == n_lines:\n",
    "        print(\"Check table size:  passed!\")    \n",
    "    else:\n",
    "        print(\"Check table size:  failed.\")\n",
    "    \n",
    "\n",
    "def check_table_lines(fpath_database=FP_DTB, fpath_table=FP_FRG):\n",
    "    \"\"\"check the integrity of the table creation\"\"\"\n",
    "    \n",
    "    def get_line_from_database():\n",
    "        \"\"\"generator of lines from database\"\"\"\n",
    "        with sqlite3.connect(fpath_database) as conn:\n",
    "            ### initiation\n",
    "            cursor = conn.cursor()\n",
    "            query  = \"select * from Fragment\"\n",
    "\n",
    "            ### get the table size of table\n",
    "            cursor = cursor.execute(query)\n",
    "            for line in cursor:\n",
    "                yield line\n",
    "                \n",
    "    def get_line_from_file():    \n",
    "        \"\"\"generator of lines from file\"\"\"\n",
    "        with gzip.open(fpath_table, \"rb\") as file:\n",
    "            lines = gen_line(file)\n",
    "            for line in lines:\n",
    "                yield line\n",
    "    \n",
    "    ### compare line by line\n",
    "    lines_base = get_line_from_database()\n",
    "    lines_file = get_line_from_file()\n",
    "    fun = lambda x, y: str(x) == str(y)\n",
    "    \n",
    "    for line_base, line_file in zip(lines_base, lines_file):\n",
    "        ### compare elements for each pair of lines and \n",
    "        ### return a list of true/false whether each pair of elements are equal\n",
    "        res = list(it.starmap(fun, zip(line_base, line_file)))\n",
    "        \n",
    "        ### if all elements are equal for each line, continue\n",
    "        ### if not, exit the function and print the failed message\n",
    "        if all(res):\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Check table lines: failed.\")\n",
    "            return\n",
    "        \n",
    "    ### all lines are equal\n",
    "    print(\"Check table lines: passed!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a62b1",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41044ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lines        = 10\n",
    "n_chunksize    = 3\n",
    "fpath_database = FP_DTB\n",
    "fpath_table    = FP_FRG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f8cd1",
   "metadata": {},
   "source": [
    "**Fragment as Primary Key; insert line by line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a2693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check table size:  passed!\n",
      "Check table lines: passed!\n"
     ]
    }
   ],
   "source": [
    "insert_line_by_line(n_lines, query_table_frag, fpath_database, fpath_table)\n",
    "\n",
    "check_table_size(n_lines)\n",
    "check_table_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ac95b",
   "metadata": {},
   "source": [
    "**Fragment as Primary Key; insert line by chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50d1a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check table size:  passed!\n",
      "Check table lines: passed!\n"
     ]
    }
   ],
   "source": [
    "insert_line_by_chunk(n_lines, n_chunksize, query_table_frag, fpath_database, fpath_table)\n",
    "\n",
    "check_table_size(n_lines)\n",
    "check_table_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e17f6",
   "metadata": {},
   "source": [
    "**Primary key is autogenerated; insert line by line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d027271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check table size:  passed!\n",
      "Check table lines: passed!\n"
     ]
    }
   ],
   "source": [
    "insert_line_by_line(n_lines, query_table_auto, fpath_database, fpath_table)\n",
    "\n",
    "check_table_size(n_lines)\n",
    "check_table_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13f8c7",
   "metadata": {},
   "source": [
    "**Primary key is autogenerated; insert line by chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758ae65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check table size:  passed!\n",
      "Check table lines: passed!\n"
     ]
    }
   ],
   "source": [
    "insert_line_by_chunk(n_lines, n_chunksize, query_table_auto, fpath_database, fpath_table)\n",
    "\n",
    "check_table_size(n_lines)\n",
    "check_table_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592dca4",
   "metadata": {},
   "source": [
    "## Profile function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e17acf-6ee4-4b03-96fe-637a5705a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lines        = 100000\n",
    "n_chunksize    = 1000\n",
    "fpath_database = FP_DTB\n",
    "fpath_table    = FP_FRG\n",
    "query_table    = query_table_frag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173af11e-3065-4cde-b427-3ec19630c8dc",
   "metadata": {},
   "source": [
    "**Profile insertion line by line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d54219c-c67a-488a-a7f7-89e012d6b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1011912 function calls in 1.415 seconds\n",
      "\n",
      "   Random listing order was used\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      794    0.001    0.000    0.001    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "   100000    0.084    0.000    0.160    0.000 {method 'readline' of '_io.BufferedReader' objects}\n",
      "     4764    0.001    0.000    0.001    0.000 {built-in method builtins.len}\n",
      "   100000    0.065    0.000    0.065    0.000 {method 'split' of 'str' objects}\n",
      "   100000    0.022    0.000    0.022    0.000 {method 'join' of 'str' objects}\n",
      "   100000    0.021    0.000    0.021    0.000 {method 'strip' of 'str' objects}\n",
      "      794    0.001    0.000    0.001    0.000 {method 'cast' of 'memoryview' objects}\n",
      "   100000    0.040    0.000    0.040    0.000 {method 'decode' of 'bytes' objects}\n",
      "   100000    0.180    0.000    0.329    0.000 /tmp/ipykernel_49165/1093682430.py:16(prep_line)\n",
      "        1    0.152    0.152    1.415    1.415 /tmp/ipykernel_49165/3568756936.py:1(fun_insert_line_by_line)\n",
      "   100000    0.084    0.000    0.319    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:372(readline)\n",
      "      794    0.006    0.000    0.069    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:438(read)\n",
      "      794    0.002    0.000    0.012    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:489(_add_read_data)\n",
      "      794    0.004    0.000    0.005    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:80(read)\n",
      "      794    0.001    0.000    0.001    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:93(prepend)\n",
      "   100000    0.036    0.000    0.036    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:298(closed)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   100000    0.614    0.000    0.614    0.000 {method 'execute' of 'sqlite3.Cursor' objects}\n",
      "   100000    0.040    0.000    0.076    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/_compression.py:12(_check_not_closed)\n",
      "      794    0.005    0.000    0.076    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/_compression.py:66(readinto)\n",
      "      794    0.044    0.000    0.044    0.000 {method 'decompress' of 'zlib.Decompress' objects}\n",
      "      794    0.010    0.000    0.010    0.000 {built-in method zlib.crc32}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### init a new table\n",
    "refresh(query_table, fpath_database)\n",
    "\n",
    "with sqlite3.connect(fpath_database) as conn:\n",
    "    ### initiation\n",
    "    cursor = conn.cursor()\n",
    "    query  = query_insert\n",
    "\n",
    "    ### read file\n",
    "    with gzip.open(fpath_table, \"rb\") as file:\n",
    "\n",
    "        ### generate lines\n",
    "        lines = gen_line(file, n_lines=n_lines)\n",
    "\n",
    "        ### start the profiler\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "\n",
    "        ### execution\n",
    "        cursor = fun_insert_line_by_line(cursor, query, lines)\n",
    "\n",
    "        ### end the profiler\n",
    "        pr.disable()\n",
    "        ps = pstats.Stats(pr).print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d873bf9-4419-4f7d-860b-3acc3ef7a236",
   "metadata": {},
   "source": [
    "**Profile insertion by chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d6f3f63-7e5e-41de-a57e-4eebb3470a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         912114 function calls in 1.187 seconds\n",
      "\n",
      "   Random listing order was used\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      794    0.002    0.000    0.002    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "   100000    0.060    0.000    0.135    0.000 {method 'readline' of '_io.BufferedReader' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "     4764    0.001    0.000    0.001    0.000 {built-in method builtins.len}\n",
      "   100000    0.060    0.000    0.060    0.000 {method 'split' of 'str' objects}\n",
      "   100000    0.020    0.000    0.020    0.000 {method 'join' of 'str' objects}\n",
      "   100000    0.017    0.000    0.017    0.000 {method 'strip' of 'str' objects}\n",
      "      794    0.001    0.000    0.001    0.000 {method 'cast' of 'memoryview' objects}\n",
      "   100000    0.029    0.000    0.029    0.000 {method 'decode' of 'bytes' objects}\n",
      "      101    0.087    0.001    0.736    0.007 /tmp/ipykernel_49165/1093682430.py:6(get_chunks)\n",
      "   100000    0.248    0.000    0.374    0.000 /tmp/ipykernel_49165/1093682430.py:16(prep_line)\n",
      "        1    0.019    0.019    1.187    1.187 /tmp/ipykernel_49165/3568756936.py:7(fun_insert_line_by_chunk)\n",
      "   100000    0.075    0.000    0.276    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:372(readline)\n",
      "      794    0.006    0.000    0.069    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:438(read)\n",
      "      794    0.002    0.000    0.012    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:489(_add_read_data)\n",
      "      794    0.004    0.000    0.005    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:80(read)\n",
      "      794    0.001    0.000    0.001    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:93(prepend)\n",
      "   100000    0.029    0.000    0.029    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/gzip.py:298(closed)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "      100    0.431    0.004    0.431    0.004 {method 'executemany' of 'sqlite3.Cursor' objects}\n",
      "   100000    0.036    0.000    0.065    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/_compression.py:12(_check_not_closed)\n",
      "      794    0.005    0.000    0.075    0.000 /nfs/software/helmod/apps/Core/Anaconda3/2019.10-gcb02/x/lib/python3.7/_compression.py:66(readinto)\n",
      "      794    0.045    0.000    0.045    0.000 {method 'decompress' of 'zlib.Decompress' objects}\n",
      "      794    0.009    0.000    0.009    0.000 {built-in method zlib.crc32}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### init a new table\n",
    "refresh(query_table, fpath_database)\n",
    "\n",
    "with sqlite3.connect(fpath_database) as conn:\n",
    "    ### initiation\n",
    "    cursor = conn.cursor()\n",
    "    query  = query_insert\n",
    "\n",
    "    ### read file\n",
    "    with gzip.open(fpath_table, \"rb\") as file:\n",
    "\n",
    "        ### generate chunks\n",
    "        chunks = gen_line(file, n_chunksize=n_chunksize, n_lines=n_lines)\n",
    "\n",
    "        ### start the profiler\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "\n",
    "        ### execution\n",
    "        cursor = fun_insert_line_by_chunk(cursor, query, chunks)\n",
    "\n",
    "        ### end the profiler\n",
    "        pr.disable()\n",
    "        ps = pstats.Stats(pr).print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be257cc7-59a3-4f73-802c-9d4bfae3e82f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
