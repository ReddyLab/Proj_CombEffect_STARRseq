{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde1b012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in: Singularity | singularity_proj_combeffect\n",
      "    BASE DIRECTORY:     /mount/work\n",
      "    PATH OF SOURCE:     /mount/work/source\n",
      "    PATH OF EXECUTABLE: /mount/work/exe\n",
      "    PATH OF ANNOTATION: /mount/work/annotation\n",
      "    PATH OF PROJECT:    /mount/project\n",
      "    PATH OF RESULTS:    /mount/work/out/proj_combeffect\n",
      "\n",
      "Library imported:\n",
      "    numpy, pandas, matplotlib.pyplot\n",
      "    os, sys, time, gzip, glob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from config_sing import *\n",
    "show_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6802a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from sklearn.manifold import TSNE\n",
    "from functools import partial, reduce\n",
    "print = partial(print, flush=True)\n",
    "\n",
    "### Set Samples\n",
    "fun = np.core.defchararray.add\n",
    "idx = np.arange(1,6).astype(\"str\")\n",
    "\n",
    "INPUT    = reduce(fun, [\"Input\", idx])\n",
    "INPUT20X = reduce(fun, [\"Input\", idx,     \"_20x\"])\n",
    "TFX_DMSO = reduce(fun, [\"TFX\",   idx[1:], \"_DMSO\"])\n",
    "TFX_DEX  = reduce(fun, [\"TFX\",   idx[1:], \"_Dex\"])\n",
    "SAMPLES  = np.concatenate([INPUT20X, TFX_DMSO, TFX_DEX])\n",
    "SAMPLES_OUT = np.concatenate([TFX_DMSO, TFX_DEX])\n",
    "GROUPS   = [\"Input\", \"Input_20x\", \"TFX_DMSO\", \"TFX_Dex\"]\n",
    "\n",
    "### file path of database\n",
    "fdiry = os.path.join(FD_RES, 'database')\n",
    "fname = \"fragment_chr17.db\"\n",
    "FPATH_DB = os.path.join(fdiry, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9189899c",
   "metadata": {},
   "source": [
    "## Test streaming fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae62d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_line(line):\n",
    "    cnames = (\n",
    "        \"Frag_Chrom\", \n",
    "        \"Frag_Start\", \n",
    "        \"Frag_End\", \n",
    "        \"Frag_Count\", \n",
    "        \"Region_Chrom\", \n",
    "        \"Region_Start\", \n",
    "        \"Region_End\",\n",
    "        \"Region_Name\", \n",
    "        \"Region_Score\",\n",
    "        \"Region_Strand\",\n",
    "        \"Overlap\")\n",
    "    lst = line.decode('ASCII').strip().split('\\t')  \n",
    "    dct = dict(zip(cnames, lst))\n",
    "    fragment = \"_\".join([dct[\"Frag_Chrom\"],   dct[\"Frag_Start\"],   dct[\"Frag_End\"]]) \n",
    "    region   = \"_\".join([dct[\"Region_Chrom\"], dct[\"Region_Start\"], dct[\"Region_End\"]]) \n",
    "    return fragment, region\n",
    "\n",
    "### helper function to get a chunk of file\n",
    "def get_chunks(gen, rows=10):\n",
    "    \"\"\"Divides the data into #rows in each list\"\"\"\n",
    "    iterable = iter(gen)\n",
    "    while True:\n",
    "        x = list(it.islice(iterable, rows))\n",
    "        if not x:\n",
    "            return\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f21c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mount/work/out/proj_combeffect/count_fragment/TFX2_DMSO/region_dex_GR_P300_dnase_chr17.bed.gz\n",
      "b'chr17\\t6001570\\t6002624\\t2\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t63\\n'\n",
      "b'chr17\\t6001571\\t6002624\\t1\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t63\\n'\n",
      "b'chr17\\t6001762\\t6002691\\t2\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t130\\n'\n",
      "b'chr17\\t6001762\\t6002692\\t1\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t131\\n'\n",
      "b'chr17\\t6001763\\t6002692\\t1\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t131\\n'\n",
      "b'chr17\\t6001765\\t6002692\\t1\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t131\\n'\n",
      "b'chr17\\t6001969\\t6002895\\t1\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t334\\n'\n",
      "b'chr17\\t6001969\\t6002896\\t1\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t335\\n'\n",
      "b'chr17\\t6001970\\t6002896\\t1\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t335\\n'\n",
      "b'chr17\\t6002054\\t6002943\\t2\\tchr17\\t6002561\\t6003866\\tchr17:6002561-6003866\\t.\\t.\\t382\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chr17_6001570_6002624',\n",
       " 'chr17_6001571_6002624',\n",
       " 'chr17_6001762_6002691',\n",
       " 'chr17_6001762_6002692',\n",
       " 'chr17_6001763_6002692',\n",
       " 'chr17_6001765_6002692',\n",
       " 'chr17_6001969_6002895',\n",
       " 'chr17_6001969_6002896',\n",
       " 'chr17_6001970_6002896',\n",
       " 'chr17_6002054_6002943']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdiry = os.path.join(FD_RES, \"count_fragment\", \"TFX2_DMSO\")\n",
    "fname = \"region_dex_GR_P300_dnase_chr17.bed.gz\"\n",
    "fpath = os.path.join(fdiry, fname)\n",
    "print(fpath)\n",
    "\n",
    "frgs = []\n",
    "with gzip.open(fpath, 'rb') as file:\n",
    "    chunks = get_chunks(file)\n",
    "    chunk  = next(chunks)\n",
    "    for line in chunk:\n",
    "        print(line)\n",
    "        frg, reg = prep_line(line)\n",
    "        frgs.append(frg)\n",
    "        \n",
    "frgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9eea4",
   "metadata": {},
   "source": [
    "## Annotation: depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2ecf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get locations from \n",
    "def get_locs(frg):\n",
    "    chrom, start, end = frg.split(\"_\")\n",
    "    loc = (int(start) + int(end)) // 2\n",
    "    return loc\n",
    "\n",
    "def get_depth(frgs, fpath_db = FPATH_DB):\n",
    "    \"\"\"get\"\"\"\n",
    "    ### get location\n",
    "    locs = list(map(get_locs, frgs))\n",
    "    \n",
    "    ### set query\n",
    "    txt   = ', '.join('?' for _ in locs)\n",
    "    query = f\"\"\"\n",
    "        SELECT   Cov.location, Cov.sample, Cov.depth, Sam.treatment, Sam.size\n",
    "        FROM     Coverage Cov\n",
    "        JOIN     Sample   Sam \n",
    "        ON       Cov.sample = Sam.sample\n",
    "        WHERE    Cov.location IN ({txt})\n",
    "        ORDER BY Cov.location\n",
    "        \"\"\"\n",
    "    \n",
    "    ### query out from database\n",
    "    with sqlite3.connect(fpath_db) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor = cursor.execute(query, locs)\n",
    "    \n",
    "    ### generate each row\n",
    "    for row in cursor:\n",
    "        grp = \"loc\", \"sam\", \"depth\", \"trt\", \"size\"\n",
    "        dct = {k:v for k, v in zip(grp, row)}\n",
    "        yield dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8da8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TFX2_DMSO', 'TFX3_DMSO', 'TFX4_DMSO', 'TFX5_DMSO', 'TFX2_Dex',\n",
       "       'TFX3_Dex', 'TFX4_Dex', 'TFX5_Dex'], dtype='<U29')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLES_OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb16cd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mount/work/out/proj_combeffect/count_fragment/TFX2_DMSO/region_dex_GR_P300_dnase_chr17.bed.gz\n",
      "/mount/work/out/proj_combeffect/scratch/region_dex_GR_P300_dnase_chr17/TFX2_DMSO_depth.tsv\n",
      "/mount/work/out/proj_combeffect/count_fragment/TFX3_DMSO/region_dex_GR_P300_dnase_chr17.bed.gz\n",
      "/mount/work/out/proj_combeffect/scratch/region_dex_GR_P300_dnase_chr17/TFX3_DMSO_depth.tsv\n",
      "/mount/work/out/proj_combeffect/count_fragment/TFX4_DMSO/region_dex_GR_P300_dnase_chr17.bed.gz\n",
      "/mount/work/out/proj_combeffect/scratch/region_dex_GR_P300_dnase_chr17/TFX4_DMSO_depth.tsv\n",
      "/mount/work/out/proj_combeffect/count_fragment/TFX5_DMSO/region_dex_GR_P300_dnase_chr17.bed.gz\n",
      "/mount/work/out/proj_combeffect/scratch/region_dex_GR_P300_dnase_chr17/TFX5_DMSO_depth.tsv\n",
      "/mount/work/out/proj_combeffect/count_fragment/TFX2_Dex/region_dex_GR_P300_dnase_chr17.bed.gz\n",
      "/mount/work/out/proj_combeffect/scratch/region_dex_GR_P300_dnase_chr17/TFX2_Dex_depth.tsv\n",
      "/mount/work/out/proj_combeffect/count_fragment/TFX3_Dex/region_dex_GR_P300_dnase_chr17.bed.gz\n",
      "/mount/work/out/proj_combeffect/scratch/region_dex_GR_P300_dnase_chr17/TFX3_Dex_depth.tsv\n",
      "/mount/work/out/proj_combeffect/count_fragment/TFX4_Dex/region_dex_GR_P300_dnase_chr17.bed.gz\n",
      "/mount/work/out/proj_combeffect/scratch/region_dex_GR_P300_dnase_chr17/TFX4_Dex_depth.tsv\n",
      "/mount/work/out/proj_combeffect/count_fragment/TFX5_Dex/region_dex_GR_P300_dnase_chr17.bed.gz\n",
      "/mount/work/out/proj_combeffect/scratch/region_dex_GR_P300_dnase_chr17/TFX5_Dex_depth.tsv\n",
      "CPU times: user 9.31 s, sys: 2.72 s, total: 12 s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for sample in SAMPLES_OUT:\n",
    "\n",
    "    ### input file\n",
    "    fdiry = os.path.join(FD_RES, \"count_fragment\", sample)\n",
    "    fname = \"region_dex_GR_P300_dnase_chr17.bed.gz\"\n",
    "    fpath_inp = os.path.join(fdiry, fname)\n",
    "\n",
    "    ### output file\n",
    "    fdiry = os.path.join(FD_RES, \"scratch\", \"region_dex_GR_P300_dnase_chr17\")\n",
    "    fname = f\"{sample}_depth.tsv\"\n",
    "    fpath_out = os.path.join(fdiry, fname)\n",
    "\n",
    "    ### show file IO\n",
    "    print(fpath_inp)\n",
    "    print(fpath_out)\n",
    "\n",
    "    with gzip.open(fpath_inp, 'rb') as finp, open(fpath_out, 'w') as fout:\n",
    "        ### set chunks\n",
    "        chunks = get_chunks(finp)\n",
    "\n",
    "        ### loop through each chunk\n",
    "        for chunk in chunks:\n",
    "            ### get fragments\n",
    "            frgs = []\n",
    "            for line in chunk:\n",
    "                frg, reg = prep_line(line)\n",
    "                frgs.append(frg)\n",
    "            frgs = np.sort(frgs)\n",
    "            \n",
    "            ### get depth from database\n",
    "            gen = get_depth(frgs)\n",
    "            dct = defaultdict(lambda: {k:0 for k in GROUPS})\n",
    "            for row in gen:\n",
    "                loc, trt, val, size = row[\"loc\"], row[\"trt\"], row[\"depth\"], row[\"size\"]\n",
    "                cpm = float(val) * (10**6) / float(size)\n",
    "                dct[loc][trt] += cpm\n",
    "\n",
    "            ### get depth foreach fragment\n",
    "            fun  = get_locs\n",
    "            locs = map(fun, frgs)\n",
    "            for loc, frg in zip(locs, frgs):\n",
    "                ### round the values\n",
    "                tmp = dct[loc]\n",
    "                tmp = {key:np.round(val, decimals=5) for key, val in tmp.items()}\n",
    "                tmp = json.dumps(tmp)\n",
    "\n",
    "                ### output to a line\n",
    "                line = \"\\t\".join([str(frg), str(loc), tmp])\n",
    "                fout.write(line + \"\\n\")\n",
    "                #print(line)\n",
    "\n",
    "            #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4c5bb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Input\": 6.134, \"Input_20x\": 6.45, \"TFX_DMSO\": 2.598, \"TFX_Dex\": 6.497}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
